# -*- coding: utf-8 -*-
"""SpeechRecognition_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCHkstMhwkLkpVPsPVn6yxZiZKotgpq-
"""

!pip install resampy

dataset_path = "/content/drive/MyDrive/Ravdess/ravdess/audio_speech_actors_01-24"

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

import librosa
import librosa.display

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

from IPython.display import display, Audio

import pickle

from google.colab import drive
drive.mount('/content/drive')

path = []
emotions = []

dirs = os.listdir(dataset_path)
for dir in dirs :
  fname = os.listdir(os.path.join(dataset_path, dir))
  for f in fname :
    path.append(os.path.join(dataset_path, dir, f))
    emotions.append(int(f.split('.')[0].split('-')[2]))

print(path[2:6] ,'\n', emotions[2:6])

df = pd.DataFrame({'path' : path, 'emotion' : emotions})
df.head()

df.shape

df['emotion'] = df['emotion'].replace({1 : 'neutral', 2 : 'calm', 3 : 'happy', 4 : 'sad', 5 : 'angry', 6 : 'fear', 7 : 'disgust', 8 : 'surprise'})
df.tail()

sns.countplot(df['emotion'])
plt.xlabel('Count')
plt.ylabel('Emotions')
sns.despine(top = True, right = True)
plt.show()

def waveplot(data, sr, e) :
  plt.figure(figsize = (10, 3))
  plt.title(f'Waveplot for {e}', size = 15)
  librosa.display.waveshow(data, sr = sr)
  plt.show()

def spectrogram(data, sr, e) :
  plt.figure(figsize=(10,3))
  stft = librosa.stft(data)
  xdb = librosa.amplitude_to_db(abs(stft))
  plt.title(f'Spectrogram for {e}', size = 15)
  librosa.display.specshow(xdb, sr=sr, x_axis = 'time', y_axis = 'hz')
  plt.colorbar()
  plt.show()

pt = np.array((df[df['emotion'] == 'fear']['path']))
fp = pt[1]
arr, sr = librosa.load(fp)

arr

sr

waveplot(arr, sr, 'fear')

spectrogram(arr, sr, 'fear')

Audio(fp)

pt = np.array((df[df['emotion'] == 'happy']['path']))
fp = pt[1]
arr, sr = librosa.load(fp)
waveplot(arr, sr, 'happy')
spectrogram(arr, sr, 'happy')

import resampy

pt = np.array((df[df['emotion'] == 'happy']['path']))
hp = pt[1]
arr, sr = librosa.load(hp, offset=0.5, sr=22050, duration=2.5)  # res_type = 'kaiser_fast'
mfcc = librosa.feature.mfcc(y=arr, sr=sr, n_mfcc=13)
happy = np.mean(mfcc, axis=0)

pt = np.array((df[df['emotion'] == 'angry']['path']))
ap = pt[1]
arr, sr = librosa.load(ap, offset = 0.5, sr = 22050, duration=2.5)   # res_type = 'kaiser_fast'
mfcc = librosa.feature.mfcc(y=arr, sr=sr, n_mfcc=13)
angry = np.mean(mfcc, axis = 0)

plt.figure(figsize = (15,5))
plt.plot(happy, label = 'happy')
plt.plot(angry, label = 'angry')
plt.legend()
plt.show()

# data augmentation
def noise(data) :
  noise_amp = 0.035*np.random.uniform()*np.amax(data)
  noised = data + noise_amp*np.random.normal(size = data.shape[0])
  return noised

def stretch(data, rate = 0.8) :
  return librosa.effects.time_stretch(data, rate = rate)

def pitch(data, sr, rate = 0.7, n_steps=None) :
  return librosa.effects.pitch_shift(data, sr = sr, n_steps = rate)

def shift(data) :
  range = int(np.random.uniform(low = -5, high = 5)*10)
  return np.roll(data, range)

path = df['path'][1]
data, sr = librosa.load(path)

noised_data = noise(data)
stretch_data = stretch(data)
pitch_data = pitch(data, sr, n_steps=4)
shift_data = shift(data)

plt.figure(figsize = (14,4))
librosa.display.waveshow(data, sr = sr)
plt.title('Original')
Audio(path)

plt.figure(figsize = (14,4))
librosa.display.waveshow(noised_data, sr = sr)
plt.title('Noised')
Audio(noised_data, rate = sr)

plt.figure(figsize = (14,4))
librosa.display.waveshow(stretch_data, sr = sr)
plt.title('Stretched')
Audio(stretch_data, rate = sr)

plt.figure(figsize = (14,4))
librosa.display.waveshow(pitch_data, sr = sr)
plt.title('Pitch')
Audio(pitch_data, rate = sr)

plt.figure(figsize = (14,4))
librosa.display.waveshow(shift_data, sr = sr)
plt.title('Shifted')
Audio(shift_data, rate = sr)

"""__Model Building__

__Trying Different Classifiers__
"""

# Features Extraction
def extract_features(data, sr) :
  result = np.array([])
  # ZCR
  zcr = np.mean(librosa.feature.zero_crossing_rate(data).T, axis = 0)
  result = np.hstack((result, zcr))

  # chroma stft
  stft = np.abs(librosa.stft(data))
  chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr = sr).T, axis = 0)
  result = np.hstack((result, chroma_stft))

  # mfcc
  mfcc = np.mean(librosa.feature.mfcc(y = data, sr = sr).T, axis = 0)
  result = np.hstack((result, mfcc))

  # rms value
  rms = np.mean(librosa.feature.rms(y = data).T, axis = 0)
  result = np.hstack((result, rms))

  # mel spectrogram
  mel = np.mean(librosa.feature.melspectrogram(y = data, sr = sr).T, axis = 0)
  result = np.hstack((result, mel))

  return result

def get_features(audio) :
  arr, sr = librosa.load(audio, duration = 2.5, offset = 0.6)
  result = np.array(extract_features(arr, sr))

  noisy = noise(arr)
  result = np.vstack((result, extract_features(noisy, sr)))

  pitchy = pitch(stretch(arr), sr)
  result = np.vstack((result, extract_features(pitchy, sr)))

  return result

get_features(df['path'][1]).shape

X, y = [], []
for p, e in zip(df['path'], df['emotion']) :
  f = get_features(p)
  for ele in f :
    X.append(ele)
    y.append(e)
len(X), len(y)

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.1, random_state = 11, shuffle = True)
len(xtrain), len(xtest)

scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)

with open('scaler.pkl', 'wb') as f:
  pickle.dump(scaler, f)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
ytrain = le.fit_transform(ytrain)
ytest = le.transform(ytest)

le.classes_

def train_pred(models, xtrain, ytrain, xtest, ytest) :
  acc = []
  ypreds = []
  for model in models :
    model.fit(xtrain, ytrain)
    pred = model.predict(xtest)
    acc.append(accuracy_score(ytest, pred))
    ypreds.append(pred)
  return acc, ypreds

logistic = LogisticRegression()
rf = RandomForestClassifier()
xgb = XGBClassifier()
svc = SVC()
ada = AdaBoostClassifier()

models = [logistic, rf, xgb, svc, ada]
model_names = ['Logistic Regression', 'Random Forest', 'XGBoost', 'SVC', 'AdaBoost']

acc, ypreds = train_pred(models, xtrain, ytrain, xtest, ytest)

for i in range(len(acc)) :
  print(f"Model : {model_names[i] : <25} Accuracy : {acc[i] : .2%} \n")

from sklearn.neighbors import KNeighborsClassifier
x = train_pred([KNeighborsClassifier()], xtrain, ytrain, xtest, ytest)
print(f"Model : K Neighbours Classifier {'' : <25} Accuracy : {x[0][0] : .2%}")

from sklearn.model_selection import RandomizedSearchCV
p = {
    'n_estimators' : [500],
    'max_depth' : [None],
    'min_samples_split' : [100, 150],
    'bootstrap' : [True, False]
}
model = RandomForestClassifier(random_state = 25)
gs = RandomizedSearchCV(estimator = model, n_jobs = -1, cv = 5, param_distributions = p)
gs.fit(xtrain, ytrain)
print(gs.best_params_)
print(gs.best_score_)
print(accuracy_score(ytest, gs.predict(xtest)))

from sklearn.ensemble import StackingClassifier
base = [
    ('rf', RandomForestClassifier(random_state = 11)),
    ('xgb', XGBClassifier(random_state = 11)),
    ('knn', KNeighborsClassifier())
]
meta = LogisticRegression()
stack = StackingClassifier(estimators = base, final_estimator = meta, cv = 5, n_jobs = -1)
stack.fit(xtrain, ytrain)
ypred = stack.predict(xtest)
a = accuracy_score(ypred, ytest)
print(f"Model : Stack {'' : <25} Accuracy : {a : .2%} \n")



cm = confusion_matrix(ytest, ypred)
cm = pd.DataFrame(cm, columns = le.classes_, index = le.classes_)
plt.figure(figsize=(15,7))
sns.heatmap(cm, annot = True, fmt = '.2f', cmap = 'Blues')
plt.show()

print(classification_report(ytest, ypred))

with open('xtrain.pkl', 'wb') as f :
  pickle.dump(xtrain, f)

with open('xtest.pkl', 'wb') as f :
  pickle.dump(xtest, f)

with open('ytrain.pkl', 'wb') as f :
  pickle.dump(ytrain, f)

with open('ytest.pkl', 'wb') as f :
  pickle.dump(ytest, f)

model_data = {
        'classifier' : stack,
        'label_encoder' : le
    }
with open('stackclassifier.pkl', 'wb') as f :
  pickle.dump(model_data, f)

model_data = {
    'classifier' : xgb,
    'label_encoder' : le
}
with open('xgbclassifier.pkl', 'wb') as f :
  pickle.dump(model_data, f)

with open('le.pkl', 'wb') as f :
  pickle.dump(le, f)

"""__Trying Neural Networks (Deep Learning)__"""

xtrain = pickle.load(open('xtrain.pkl', 'rb'))
xtest = pickle.load(open('xtest.pkl', 'rb'))
ytrain = pickle.load(open('ytrain.pkl', 'rb'))
ytest = pickle.load(open('ytest.pkl', 'rb'))

one = OneHotEncoder()
ytrain_encoded = one.fit_transform(ytrain.reshape(-1,1)).toarray()
ytest_encoded = one.transform(ytest.reshape(-1,1)).toarray()

# Reshape data for lstm (samples, timesteps, features)
def prepare(x, timesteps = 5) :
  x_reshaped = []
  for i in range(len(x) - timesteps + 1) :
    x_reshaped.append(x[i : i + timesteps])
  return np.array(x_reshaped)

timesteps = 5
xtrain_reshaped = prepare(xtrain)
xtest_reshaped = prepare(xtest)
ytrain_reshaped = ytrain_encoded[timesteps - 1 :]
ytest_reshaped = ytest_encoded[timesteps - 1 :]

model_lstm = Sequential([
    Bidirectional(LSTM(128, activation = 'relu', return_sequences = True, input_shape = (xtrain_reshaped.shape[1], xtrain_reshaped.shape[2]))),
    BatchNormalization(),
    Dropout(0.2),

    Bidirectional(LSTM(64, activation = 'relu')),
    BatchNormalization(),
    Dropout(0.2),

    Dense(32, activation='relu'),
    BatchNormalization(),

    Dense(8, activation='softmax')
])

model_lstm.compile(optimizer = Adam(learning_rate = 0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])
model_lstm.summary()

es = EarlyStopping(monitor = 'val_loss', patience = 20, min_delta=0.001, restore_best_weights = True)
rdlr = ReduceLROnPlateau(monitor = 'val_loss', patience = 10, min_lr=0.0001, factor = 0.2, cooldown = 1)

history = model_lstm.fit(xtrain_reshaped, ytrain_reshaped, validation_data = (xtest_reshaped, ytest_reshaped), epochs = 100, batch_size = 32, verbose = 1, callbacks = [es, rdlr])

l , ac = model_lstm.evaluate(xtest_reshaped, ytest_reshaped)
print("Loss: ", l)
print("Accuracy: ", ac)

yp = model_lstm.predict(xtest_reshaped)
yp = np.argmax(yp, axis = 1)
true = np.argmax(ytest_reshaped, axis = 1)
print(classification_report(true, yp))

cm = confusion_matrix(true, yp)
d = pd.DataFrame(cm, columns = le.classes_, index = le.classes_)
plt.figure(figsize=(15,7))
sns.heatmap(d, annot = True, fmt = '.2f', cmap = 'Blues')
plt.show()

with open('lstm.pkl', 'wb') as f :
  pickle.dump(model_lstm, f)

model_lstm.save('lstm_speech.h5')  # Recommended format for Keras models

q = np.random.rand(1, 6, 512).astype(np.float32)
q.shape